<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Proxy | Chaker Benhamed]]></title>
  <link href="http://blog.chaker.tn/blog/categories/proxy/atom.xml" rel="self"/>
  <link href="http://blog.chaker.tn/"/>
  <updated>2015-07-17T16:30:34+01:00</updated>
  <id>http://blog.chaker.tn/</id>
  <author>
    <name><![CDATA[Chaker Benhamed]]></name>
    <email><![CDATA[mail@chaker.tn]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Creating Basic Proxy Checker With Python]]></title>
    <link href="http://blog.chaker.tn/blog/creating-basic-proxy-checker-with-python/"/>
    <updated>2015-07-12T11:03:54+01:00</updated>
    <id>http://blog.chaker.tn/blog/creating-basic-proxy-checker-with-python</id>
    <content type="html"><![CDATA[<p>I&#39;m in love with web scrapping but sometimes I get banned by the site that I wanna extract the data from. So the first and basic solution is to use proxies. However finding a working proxy in a proxy list is pain in the neck. That&#39;s why I decided to create my own &quot;basic&quot; proxy checker.</p>

<!-- more -->

<h2>The idea:</h2>

<p>It&#39;s very simple, all we going to do is to test our proxies:</p>

<ol>
<li>Test connectivity.</li>
<li>Test timeout.</li>
<li>Check if the proxy is returning the right page or not.</li>
</ol>

<h2>What I need:</h2>

<ul>
<li>Python: DAHH!!!</li>
<li>We gonna use <a href="http://docs.python-requests.org/en/latest/">requests</a> module for the HTTP requests. If you don&#39;t already have it you should take a look. And if you don&#39;t want to install it you can simply follow on but you have to adapt the function to your library of choice.</li>
<li>Proxy list to test. If you don&#39;t have one just google it.</li>
</ul>

<h2>Check a single proxy:</h2>

<p>First let&#39;s start by creating a function that test a single proxy. The function will load a webpage and returns False if there are error during the request. Then checks if the title is in the HTML or not and then returns the result.</p>

<p><code>python Check single proxy 
def checkproxy(proxy, timeout = 5, url=&quot;http://example.com&quot;, title=&quot;&lt;title&gt;Example Domain&lt;/title&gt;&quot;):
    proxies = {
            &quot;http&quot; : &quot;http://&quot; + proxy,
            &quot;https&quot; : &quot;https://&quot; +proxy
            } # Prepare the proxy
    try : # Try to make a GET Request to URL.
        r = requests.get(url,proxies=proxies, timeout=timeout)
    except : # If any error occured (Including timeout) return False.
        return False
    return title in r.text # Else check if title is in the HTML and return the result.
</code>
By default this function will try to connect to <code>http://example.com</code> with a timeout of <code>5</code> seconds. If every thing passed OK it will check the presence of <code>&lt;title&gt;Example Domain&lt;/title&gt;</code> and returns the result. </p>

<p>OK Now we have a function that test if a given proxy is valid or not. But I want to check a proxy list? So we need to speed things up. Since checking every proxy aside will take time. We should try threads.</p>

<h2>Using threads:</h2>

<p>Checking a proxy list with a single thread is very slow. But implementing threaded solution to this problem is not quite straightforward. So I did a little search and I found <a href="http://stackoverflow.com/a/2635066/4237058">this</a>. And with some modification I ended up with this:</p>

<p><div><script src='https://gist.github.com/e47e21aa36995783b690.js'></script>
<noscript><pre><code>import requests
from threading import Thread
from Queue import Queue
import sys

def checkproxy(proxy, timeout = 5, url=&quot;http://example.com&quot;, title=&quot;&lt;title&gt;Example Domain&lt;/title&gt;&quot;):
    &quot;&quot;&quot;
    Return True if proxy is alive, False if not
    Usage :
    checkproxy(proxy)
    checkproxy(proxy,10,'http://google.com','&lt;title&gt;Google&lt;/title&gt;')
    &quot;&quot;&quot;
    proxies = {
            &quot;http&quot; : &quot;http://&quot; + proxy,
            &quot;https&quot; : &quot;https://&quot; +proxy
            }
    try :
        r = requests.get(url,proxies=proxies, timeout=timeout)
    except :
        return False
    return title in r.text


concurrent = 200

number_lines = 0
number_valid = 0

def doSomethingWithResult(status, proxy):
    global number_valid
    if status:
        print proxy
        number_valid +=1

def CheckProxyList():
    while True:
        proxy = q.get()
        status = checkproxy(proxy)
        doSomethingWithResult(status, proxy)
        q.task_done()


q = Queue(concurrent * 2)

for i in range(concurrent):
    t = Thread(target=CheckProxyList)
    t.daemon = True
    t.start()
try:
    for proxy in open('./proxylist.txt'):
        number_lines += 1
        q.put(proxy.strip())
    q.join()

    print &quot;[+] Checked {}&quot;.format(number_lines)
    print &quot;[+] Valid {}&quot;.format(number_valid)
except KeyboardInterrupt:
    print 
    print &quot;Exiting&quot;
    sys.exit(1)</code></pre></noscript></div>
</p>

<p>So I run it on on <code>6k</code> proxy list and It&#39;s reasonably fast. </p>

<p>This script will print the valid proxy to <code>stdout</code>. But it should be no problem to redirect it to file or to modify it to write it directly to a file. Feel free to modify and/or to use the script. And don&#39;t forget to comment below.</p>
]]></content>
  </entry>
  
</feed>
